size = EQ_PRIMARY
)) +
geom_timeline() +
theme_timeline()
source('~/datasciencecoursera/noaa.R')
noaa %>% eq_clean_data() %>%
filter(COUNTRY %in% c("GREECE", "ITALY", "USA"), YEAR > 2000) %>%
ggplot(aes(x = DATE,
y = COUNTRY,
color = TOTAL_DEATHS,
size = EQ_PRIMARY
)) +
geom_timeline() +
theme_timeline()
source('~/datasciencecoursera/noaa.R')
noaa %>% eq_clean_data() %>%
filter(COUNTRY %in% c("GREECE", "ITALY", "USA"), YEAR > 2000) %>%
ggplot(aes(x = DATE,
y = COUNTRY,
color = TOTAL_DEATHS,
size = EQ_PRIMARY
)) +
geom_timeline() +
theme_timeline()
source('~/datasciencecoursera/noaa.R')
noaa %>% eq_clean_data() %>%
filter(COUNTRY %in% c("GREECE", "ITALY", "USA"), YEAR > 2000) %>%
ggplot(aes(x = DATE,
y = COUNTRY,
color = TOTAL_DEATHS,
size = EQ_PRIMARY
)) +
geom_timeline() +
theme_timeline()
source('~/datasciencecoursera/noaa.R')
noaa %>% eq_clean_data() %>%
filter(COUNTRY %in% c("GREECE", "ITALY", "USA"), YEAR > 2000) %>%
ggplot(aes(x = DATE,
y = COUNTRY,
color = TOTAL_DEATHS,
size = EQ_PRIMARY
)) +
geom_timeline() +
theme_timeline()
source('~/datasciencecoursera/noaa.R')
noaa %>% eq_clean_data() %>%
filter(COUNTRY %in% c("GREECE", "ITALY", "USA"), YEAR > 2000) %>%
ggplot(aes(x = DATE,
y = COUNTRY,
color = TOTAL_DEATHS,
size = EQ_PRIMARY
)) +
geom_timeline() +
theme_timeline()
source('~/datasciencecoursera/noaa.R')
noaa %>% eq_clean_data() %>%
filter(COUNTRY %in% c("GREECE", "ITALY", "USA"), YEAR > 2000) %>%
ggplot(aes(x = DATE,
y = COUNTRY,
color = TOTAL_DEATHS,
size = EQ_PRIMARY
)) +
geom_timeline() +
theme_timeline()
source('~/datasciencecoursera/noaa.R')
noaa %>% eq_clean_data() %>%
filter(COUNTRY %in% c("GREECE", "ITALY", "USA"), YEAR > 2000) %>%
ggplot(aes(x = DATE,
y = COUNTRY,
color = TOTAL_DEATHS,
size = EQ_PRIMARY
)) +
geom_timeline() +
theme_timeline()
source('~/datasciencecoursera/noaa.R')
noaa %>% eq_clean_data() %>%
filter(COUNTRY %in% c("GREECE", "ITALY", "USA"), YEAR > 2000) %>%
ggplot(aes(x = DATE,
y = COUNTRY,
color = TOTAL_DEATHS,
size = EQ_PRIMARY
)) +
geom_timeline() +
theme_timeline()
source('~/datasciencecoursera/noaa.R')
noaa %>% eq_clean_data() %>%
filter(COUNTRY %in% c("GREECE", "ITALY", "USA"), YEAR > 2000) %>%
ggplot(aes(x = DATE,
y = COUNTRY,
color = TOTAL_DEATHS,
size = EQ_PRIMARY
)) +
geom_timeline() +
theme_timeline()
source('~/datasciencecoursera/noaa.R')
noaa %>% eq_clean_data() %>%
filter(COUNTRY %in% c("GREECE", "ITALY", "USA"), YEAR > 2000) %>%
ggplot(aes(x = DATE,
y = COUNTRY,
color = TOTAL_DEATHS,
size = EQ_PRIMARY
)) +
geom_timeline() +
theme_timeline()
source('~/datasciencecoursera/noaa.R')
noaa %>% eq_clean_data() %>%
filter(COUNTRY %in% c("GREECE", "ITALY", "USA"), YEAR > 2000) %>%
ggplot(aes(x = DATE,
y = COUNTRY,
color = TOTAL_DEATHS,
size = EQ_PRIMARY
)) +
geom_timeline() +
theme_timeline()
source('~/datasciencecoursera/noaa.R')
source('~/datasciencecoursera/noaa.R')
noaa %>% eq_clean_data() %>%
filter(COUNTRY %in% c("GREECE", "ITALY", "USA"), YEAR > 2000) %>%
ggplot(aes(x = DATE,
y = COUNTRY,
color = TOTAL_DEATHS,
size = EQ_PRIMARY
)) +
geom_timeline() +
theme_timeline()
noaa %>% eq_clean_data() %>%
filter(COUNTRY %in% c("GREECE", "ITALY", "USA"), YEAR > 2000) %>%
ggplot(aes(x = DATE,
y = COUNTRY,
color = TOTAL_DEATHS,
size = EQ_PRIMARY
)) + labs(color="# Deaths") + labs(size="Richter scale") +
geom_timeline() +
theme_timeline()
noaa %>% eq_clean_data() %>%
filter(COUNTRY %in% c("GREECE", "ITALY", "USA"), YEAR > 2000) %>%
ggplot(aes(x = DATE,
y = COUNTRY,
color = TOTAL_DEATHS,
size = EQ_PRIMARY
)) + labs(color="# Deaths",size="Richter scale") +
geom_timeline() +
theme_timeline()
source('~/datasciencecoursera/ada.R')
source('~/datasciencecoursera/ada.R')
source('~/datasciencecoursera/ada.R')
warnings()
source('~/datasciencecoursera/ada.R')
warnings()
?train
source('~/datasciencecoursera/ada.R')
source('~/datasciencecoursera/ada.R')
warnings()
source('~/datasciencecoursera/ada.R')
warnings()
source('~/datasciencecoursera/ada.R')
source('~/datasciencecoursera/ada.R')
source('~/datasciencecoursera/ada.R')
source('~/datasciencecoursera/ada.R')
source('~/datasciencecoursera/ada.R')
source('~/datasciencecoursera/ada.R')
source('~/datasciencecoursera/ada.R')
source('~/datasciencecoursera/ada.R')
source('~/datasciencecoursera/ada.R')
rm(list=ls())
library(ElemStatLearn)
data(vowel.test)
data(vowel.train)
training<-vowel.train
testing<-vowel.test
training$y <- as.factor(training$y)
testing$y <- as.factor(testing$y)
set.seed(33833)
library(caret)
rfMod <- train(y~., method="rf", data=training)
gbmMod <- train(y~., method="gbm", data=training)
gbmMod <- train(y~., method="gbm", data=training)
gbmMod$finalModel
gbmMod <- train(y~., method="gbm", data=training)library(gbm)
library(gbm)
library(pgmm)
library(lubridate)
library(forecast)
install.packages("forecast")
library(forecast)
library(e1071)
gbmMod <- train(y~., method="gbm", data=training)
rm(list=ls())
library(AppliedPredictiveModeling); library(caret); library(ElemStatLearn); library(pgmm)
library(rpart); library(gbm); library(lubridate); library(forecast); library(e1071)
data("vowel.train")
data("vowel.test")
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
set.seed(33833)
mod1 <- train(y~., method = "rf", data = vowel.train)
mod2 <- train(y~., method = "gbm", data = vowel.train)
pred1 <- predict(mod1, vowel.test)
pred2 <- predict(mod2, vowel.test)
r1 = confusionMatrix(pred1, vowel.test$y)
r2 = confusionMatrix(pred2, vowel.test$y)
r3 = confusionMatrix(pred1, pred2)
r1
r2
r3
predDF <- data.frame(pred_rf, pred_gbm, y = vowel.test$y)
# Accuracy among the test set samples where the two methods agree
sum(pred_rf[predDF$pred_rf == predDF$pred_gbm] ==
predDF$y[predDF$pred_rf == predDF$pred_gbm]) /
sum(predDF$pred_rf == predDF$pred_gbm)
predDF <- data.frame(pred1, pred2, y = vowel.test$y)
# Accuracy among the test set samples where the two methods agree
sum(pred1[predDF$pred1 == predDF$pred2] ==
predDF$y[predDF$pred1 == predDF$pred2]) /
sum(predDF$pred1 == predDF$pred2)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
mod_rf <- train(diagnosis ~., model = "rf", data=training)
mod_gbm <- train(diagnosis ~., model = "gbm", data=training)
mod_lda <- train(diagnosis ~., model = "lda", data=training)
mod_rf
mod_rf$finalModel
pred_rf <- predict(mod_rf,testing)
pred_gbm <- predict(mod_gbm,testing)
pred_lda <- predict(mod_lda,testing)
predDF <- data.frame(pred_rf, pred_gbm, pred_lda, diagnosis = testing$diagnosis)
combModFit <- train(diagnosis ~ ., method = "rf", data = predDF)
combPred <- predict(combModFit, predDF)
set.seed(62433)
mod_rf <- train(diagnosis ~ ., data = training, method = "rf")
mod_gbm <- train(diagnosis ~ ., data = training, method = "gbm")
mod_lda <- train(diagnosis ~ ., data = training, method = "lda")
pred_rf <- predict(mod_rf, testing)
pred_gbm <- predict(mod_gbm, testing)
pred_lda <- predict(mod_lda, testing)
predDF <- data.frame(pred_rf, pred_gbm, pred_lda, diagnosis = testing$diagnosis)
combModFit <- train(diagnosis ~ ., method = "rf", data = predDF)
combPred <- predict(combModFit, predDF)
confusionMatrix(pred_rf, testing$diagnosis)$overall[1]
confusionMatrix(pred_gbm, testing$diagnosis)$overall[1]
confusionMatrix(pred_lda, testing$diagnosis)$overall[1]
confusionMatrix(pred_lda, testing$diagnosis)$overall[1]
confusionMatrix(combPred, testing$diagnosis)$overall[1]
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
mod_lasso = train(CompressiveStrength ~ .,
data = training,
method = "lasso")
mod_lasso = train(CompressiveStrength ~ .,
data = training,
method = "lasso")
plot.enet(mod_lasso$finalModel,
xvar="penalty", use.color=TRUE)
library(lubridate)
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
mod_ts <- bats(tstrain)
fcast <- forecast(mod_ts, level = 95, h = dim(testing)[1])
sum(fcast$lower < testing$visitsTumblr & testing$visitsTumblr < fcast$upper) /
dim(testing)[1]
?forecast
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
model <- svm(CompressiveStrength~., data=training)
predictedY <-predict(model,testing)
points(testing,predictedY, col='red', pch=4)
error<-testing$CompressedStrength - predictedY
svmRMSE <- rmse(error)
accuracy(predictedY,testing$CompressiveStrength)
?which
?substr
?rpart
?trainControl
?cor
?which
?brewer.pal
??brewer.pal
??corrplot
install.packages("corrplot")
?rpart
?expand.grid
?trainControl
?rpart
?rpart2
??rpart2
library(rpart)
?rpart2
??rpart2
?train
install.packages("cvTools")
install.packages("robustbase")
?preProcess
?gbm
??gbm
?randomForest
??randomForest
?expand.grid
?train
knitr::opts_chunk$set(echo = TRUE)
library(readr)
suppressMessages(library(caret))
library(cvTools)
library(gbm)
library(plyr)
library(randomForest)
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "pml-training.csv"
testFile  <- "pml-testing.csv"
if (!file.exists(trainFile)) {
download.file(trainUrl, destfile=trainFile, method="curl")
}
if (!file.exists(testFile)) {
download.file(testUrl, destfile=testFile, method="curl")
}
training <- read.csv(file = trainFile, header = TRUE, sep = ",")
testing  <- read.csv(file = testFile, header = TRUE, sep = ",")
dim(training); dim(testing)
training_na <- training[, colSums(is.na(training)) != 0]
na_count <-sapply(training_na, function(na) sum(is.na(na)))
na_count <- data.frame(na_count)
trainingData <- training[, colSums(is.na(training)) == 0]#removing variables with NA
trainingData <- trainingData[ ,-c(1:7)]#removing irrelevant/empty variables
classe <- trainingData$classe
trainingData <- trainingData[, sapply(trainingData, is.numeric)]
trainingData$classe <- classe
testingData <- testing[, colSums(is.na(testing)) == 0]
testingData <- testingData[ ,-c(1:7)]
testingData <- testingData[, sapply(testingData, is.numeric)]
dim(trainingData); dim(testingData)
inTrain <- createDataPartition(y=trainingData$classe, p=0.7, list=FALSE)
trainSub <- trainingData[inTrain,]
validSub <- trainingData[-inTrain,]
set.seed(123)
trCtrl <- trainControl(method = 'cv', number = 5, summaryFunction=defaultSummary)
#Grid <- expand.grid(cp = seq(0, 0.05, 0.001))#complexity parameter
fit_rpart <- train(classe~., data = trainSub, method = 'rpart', trControl = trCtrl)
print(fit_rpart)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
suppressMessages(library(caret))
library(cvTools)
library(gbm)
library(plyr)
library(randomForest)
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "pml-training.csv"
testFile  <- "pml-testing.csv"
if (!file.exists(trainFile)) {
download.file(trainUrl, destfile=trainFile, method="curl")
}
if (!file.exists(testFile)) {
download.file(testUrl, destfile=testFile, method="curl")
}
training <- read.csv(file = trainFile, header = TRUE, sep = ",")
testing  <- read.csv(file = testFile, header = TRUE, sep = ",")
dim(training); dim(testing)
training_na <- training[, colSums(is.na(training)) != 0]
na_count <-sapply(training_na, function(na) sum(is.na(na)))
na_count <- data.frame(na_count)
trainingData <- training[, colSums(is.na(training)) == 0]#removing variables with NA
trainingData <- trainingData[ ,-c(1:7)]#removing irrelevant/empty variables
classe <- trainingData$classe
trainingData <- trainingData[, sapply(trainingData, is.numeric)]
trainingData$classe <- classe
testingData <- testing[, colSums(is.na(testing)) == 0]
testingData <- testingData[ ,-c(1:7)]
testingData <- testingData[, sapply(testingData, is.numeric)]
dim(trainingData); dim(testingData)
inTrain <- createDataPartition(y=trainingData$classe, p=0.7, list=FALSE)
trainSub <- trainingData[inTrain,]
validSub <- trainingData[-inTrain,]
set.seed(123)
trCtrl <- trainControl(method = 'cv', number = 5, summaryFunction=defaultSummary)
#Grid <- expand.grid(cp = seq(0, 0.05, 0.001))#complexity parameter
fit_rpart <- train(classe~., data = trainSub, method = 'rpart', trControl = trCtrl)
print(fit_rpart)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
suppressMessages(library(caret))
library(cvTools)
library(gbm)
library(plyr)
library(randomForest)
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "pml-training.csv"
testFile  <- "pml-testing.csv"
if (!file.exists(trainFile)) {
download.file(trainUrl, destfile=trainFile, method="curl")
}
if (!file.exists(testFile)) {
download.file(testUrl, destfile=testFile, method="curl")
}
training <- read.csv(file = trainFile, header = TRUE, sep = ",")
testing  <- read.csv(file = testFile, header = TRUE, sep = ",")
dim(training); dim(testing)
training_na <- training[, colSums(is.na(training)) != 0]
na_count <-sapply(training_na, function(na) sum(is.na(na)))
na_count <- data.frame(na_count)
trainingData <- training[, colSums(is.na(training)) == 0]#removing variables with NA
trainingData <- trainingData[ ,-c(1:7)]#removing irrelevant/empty variables
classe <- trainingData$classe
trainingData <- trainingData[, sapply(trainingData, is.numeric)]
trainingData$classe <- classe
testingData <- testing[, colSums(is.na(testing)) == 0]
testingData <- testingData[ ,-c(1:7)]
testingData <- testingData[, sapply(testingData, is.numeric)]
dim(trainingData); dim(testingData)
inTrain <- createDataPartition(y=trainingData$classe, p=0.7, list=FALSE)
trainSub <- trainingData[inTrain,]
validSub <- trainingData[-inTrain,]
set.seed(123)
trCtrl <- trainControl(method = 'cv', number = 5, summaryFunction=defaultSummary)
#Grid <- expand.grid(cp = seq(0, 0.05, 0.001))#complexity parameter
fit_rpart <- train(classe~., data = trainSub, method = 'rpart', trControl = trCtrl)
print(fit_rpart)
#Grid <- expand.grid( n.trees = seq(5, 100, 5), interaction.depth = c(10), shrinkage = c(0.1), n.minobsinnode = 20)
fit_gbm <- train(classe~., data = trainSub, method = 'gbm', trControl = trCtrl, trace=FALSE)
?train
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(caret)
library(cvTools)
library(gbm)
library(plyr)
library(randomForest)
library(MASS)
#Grid <- expand.grid( n.trees = seq(5, 100, 5), interaction.depth = c(10), shrinkage = c(0.1), n.minobsinnode = 20)
fit_gbm <- train(classe~., data = trainSub, method = 'gbm', trControl = trCtrl)
#Grid <- expand.grid( n.trees = seq(5, 100, 5), interaction.depth = c(10), shrinkage = c(0.1), n.minobsinnode = 20)
fit_gbm <- train(classe~., data = trainSub, method = 'gbm', trControl = trCtrl, trace=FALSE)
rm("c")
#Grid <- expand.grid( n.trees = seq(5, 100, 5), interaction.depth = c(10), shrinkage = c(0.1), n.minobsinnode = 20)
fit_gbm <- train(classe~., data = trainSub, method = 'gbm', trControl = trCtrl, trace = FALSE)
rm("c")
?randomForest
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(caret)
library(cvTools)
library(gbm)
library(plyr)
library(randomForest)
grid <- expand.grid( mtry = 27 )
model <- train(classe~., data = trainSub, method = 'rf', tuneGrid = grid)
grid <- expand.grid( mtry = 27 )
model <- train(classe~., data = trainSub, method = 'rf', tuneGrid = grid)
grid <- expand.grid( mtry = 27 )
model <- train(classe~., data = trainSub, method = 'rf', tuneGrid = grid)
#randomForest(classe~., data=trainSub, mtry=27)
#
prediction <- predict(model, validSub)
error = 1 - sum( as.numeric(prediction) == as.numeric(validSubt$classe))/length(prediction)
ls()
error
ls ()
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(caret)
library(cvTools)
library(gbm)
library(plyr)
library(randomForest)
trCtrl <- trainControl(method = 'cv', number = 5, summaryFunction=defaultSummary)
grid <- expand.grid( n.trees = seq(50, 150, 5), interaction.depth = c(10), shrinkage = c(0.1), n.minobsinnode = 20)
fit_gbm <- train(classe~., data = trainSub, method = 'gbm', trControl = trCtrl, tuneGrid = grid, verbose = FALSE)
trCtrl <- trainControl(method = 'cv', number = 5, summaryFunction=defaultSummary)
grid <- expand.grid( n.trees = seq(50, 150, 5), interaction.depth = c(10), shrinkage = c(0.1), n.minobsinnode = 20)
fit_gbm <- train(classe~., data=trainSub, method='gbm', trControl=trCtrl, tuneGrid=grid, verbose = FALSE)
getwd()
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(caret)
library(cvTools)
library(gbm)
library(plyr)
library(randomForest)
trCtrl <- trainControl(method = 'cv', number = 5, summaryFunction=defaultSummary)
grid <- expand.grid( n.trees = seq(50, 150, 5), interaction.depth = c(10), shrinkage = c(0.1), n.minobsinnode = 20)
fit_gbm <- train(classe~., data=trainSub, method='gbm', trControl=trCtrl, tuneGrid=grid, verbose = FALSE)
shiny::runApp('disk_prices')
runApp('disk_prices')
runApp('disk_prices')
runApp('disk_prices')
runApp('disk_prices')
runApp('disk_prices')
getwd()
setwd("disk_prices")
runApp()
